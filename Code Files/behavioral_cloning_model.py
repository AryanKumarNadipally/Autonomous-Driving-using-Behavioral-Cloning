# -*- coding: utf-8 -*-
"""Behavioral Cloning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ncjLPtUi5uSHXcNx2CzvPc4oGZSI8Rzh
"""

!git clone https://github.com/AryanKumarNadipally/Autonomous-Driving-using-Behavioral-Cloning.git

!ls Autonomous-Driving-using-Behavioral-Cloning

!pip3 install imgaug

import os
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import ntpath
import random
import cv2
import matplotlib.image as mpimg
from keras.models import Sequential
from keras.optimizers import Adam
from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from imgaug import augmenters as iaa

# Set directory and column names
data_directory = 'Autonomous-Driving-using-Behavioral-Cloning'
column_names = ['center', 'left', 'right', 'steering', 'throttle', 'reverse', 'speed']

# Load and display data
drive_data = pd.read_csv(os.path.join(data_directory, 'driving_log.csv'), names = column_names)
pd.set_option('display.max_colwidth', None)
drive_data.head()

# Function to extract filename from path
def extract_filename(path):
    head, tail = ntpath.split(path)
    return tail

# Apply filename extraction
drive_data['center'] = drive_data['center'].apply(extract_filename)
drive_data['left'] = drive_data['left'].apply(extract_filename)
drive_data['right'] = drive_data['right'].apply(extract_filename)
drive_data.head()

# Steering angle analysis
num_bins = 25
max_samples = 400
steering_hist, steering_bins = np.histogram(drive_data['steering'], num_bins)
mid_bin = (steering_bins[:-1] + steering_bins[1:]) * 0.5
plt.bar(mid_bin, steering_hist, width=0.05)
plt.plot((np.min(drive_data['steering']), np.max(drive_data['steering'])), (max_samples, max_samples))

# Data balancing

print('Total data:', len(drive_data))
data_to_remove = []
for i in range(num_bins):
    bin_list = []
    for j in range(len(drive_data['steering'])):
        if steering_bins[i] <= drive_data['steering'][j] < steering_bins[i + 1]:
            bin_list.append(j)
    bin_list = shuffle(bin_list)
    bin_list = bin_list[max_samples:]
    data_to_remove.extend(bin_list)

# Histogram of balanced data
balanced_hist, _ = np.histogram(drive_data['steering'], num_bins)
plt.bar(mid_bin, balanced_hist, width=0.05)
plt.plot((np.min(drive_data['steering']), np.max(drive_data['steering'])), (max_samples, max_samples))

# Display a sample data row
print(drive_data.iloc[1])

# Function to load images and steering data
def load_images_and_steering(data_dir, dataframe):
    image_paths, steerings = [], []
    for index in range(len(dataframe)):
        record = dataframe.iloc[index]
        center, left, right = record['center'], record['left'], record['right']
        image_paths.append(os.path.join(data_dir, center))
        steerings.append(float(record['steering']))
        # Adjust steering for side images
        image_paths.append(os.path.join(data_dir, left))
        steerings.append(float(record['steering']) + 0.15)
        image_paths.append(os.path.join(data_dir, right))
        steerings.append(float(record['steering']) - 0.15)
    return np.array(image_paths), np.array(steerings)

# Load and split data
image_dir = data_directory + '/IMG'
image_paths, steer_angles = load_images_and_steering(image_dir, drive_data)
X_train, X_valid, y_train, y_valid = train_test_split(image_paths, steer_angles, test_size=0.2, random_state=6)

# Visualization of training and validation set
fig, axes = plt.subplots(1, 2, figsize=(12, 4))
axes[0].hist(y_train, bins=num_bins, width=0.05, color='blue')
axes[0].set_title('Training Set Distribution')
axes[1].hist(y_valid, bins=num_bins, width=0.05, color='red')
axes[1].set_title('Validation Set Distribution')

# Image augmentation functions
def augment_zoom(image):
    zoom_aug = iaa.Affine(scale=(1, 1.3))
    return zoom_aug.augment_image(image)

def augment_pan(image):
    pan_aug = iaa.Affine(translate_percent={"x": (-0.1, 0.1), "y": (-0.1, 0.1)})
    return pan_aug.augment_image(image)

def augment_brightness(image):
    brightness_aug = iaa.Multiply((0.2, 1.2))
    return brightness_aug.augment_image(image)

def augment_flip(image, steering_angle):
    flipped_image = cv2.flip(image, 1)
    return flipped_image, -steering_angle

# Example of augmentation
index = random.randint(0, 1000)
sample_image = mpimg.imread(image_paths[index])
fig, axs = plt.subplots(1, 4, figsize=(15, 10))
fig.tight_layout()
axs[0].imshow(sample_image)
axs[0].set_title('Original Image')
axs[1].imshow(augment_zoom(sample_image))
axs[1].set_title('Zoomed Image')
axs[2].imshow(augment_pan(sample_image))
axs[2].set_title('Panned Image')
axs[3].imshow(augment_brightness(sample_image))
axs[3].set_title('Brightness Adjusted Image')

# Combined random augmentation function
def augment_image(image_path, steering_value):
    image = mpimg.imread(image_path)
    if np.random.rand() < 0.5:
        image = augment_pan(image)
    if np.random.rand() < 0.5:
        image = augment_zoom(image)
    if np.random.rand() < 0.5:
        image = augment_brightness(image)
    if np.random.rand() < 0.5:
        image, steering_value = augment_flip(image, steering_value)
    return image, steering_value

# Image preprocessing
def preprocess_image(img):
    img = img[60:135, :, :]
    img = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)
    img = cv2.GaussianBlur(img, (3, 3), 0)
    img = cv2.resize(img, (200, 66))
    img = img / 255
    return img

# Preprocess and visualize an example image
sample_image_path = image_paths[100]
original_img = mpimg.imread(sample_image_path)
processed_img = preprocess_image(original_img)

fig, axs = plt.subplots(1, 2, figsize=(15, 10))
fig.tight_layout()
axs[0].imshow(original_img)
axs[0].set_title('Original Image')
axs[1].imshow(processed_img)
axs[1].set_title('Preprocessed Image')

# Batch generator for model training
def batch_generator(image_paths, steering_angles, batch_size, is_training):
    while True:
        batch_images = []
        batch_steering_angles = []

        for i in range(batch_size):
            index = random.randint(0, len(image_paths) - 1)
            if is_training:
                img, steering = augment_image(image_paths[index], steering_angles[index])
            else:
                img = mpimg.imread(image_paths[index])
                steering = steering_angles[index]

            img = preprocess_image(img)
            batch_images.append(img)
            batch_steering_angles.append(steering)

        yield np.array(batch_images), np.array(batch_steering_angles)

# NVIDIA model architecture
def create_nvidia_model():
    model = Sequential()
    model.add(Conv2D(24, (5, 5), strides=(2, 2), input_shape=(66, 200, 3), activation='elu'))
    model.add(Conv2D(36, (5, 5), strides=(2, 2), activation='elu'))
    model.add(Conv2D(48, (5, 5), strides=(2, 2), activation='elu'))
    model.add(Conv2D(64, (3, 3), activation='elu'))

    model.add(Conv2D(64, (3, 3), activation='elu'))
    model.add(Dropout(0.5))
    model.add(Flatten())

    model.add(Dense(100, activation='elu'))
    model.add(Dropout(0.5))

    model.add(Dense(50, activation='elu'))
    model.add(Dropout(0.5))

    model.add(Dense(10, activation='elu'))
    model.add(Dropout(0.5))

    model.add(Dense(1))
    model.compile(loss='mse', optimizer=Adam(lr=0.001))
    return model

# Create and display the model
model = create_nvidia_model()
print(model.summary())

# Model training
training_generator = batch_generator(X_train, y_train, 100, True)
validation_generator = batch_generator(X_valid, y_valid, 100, False)

training_history = model.fit(training_generator,
                             steps_per_epoch=300,
                             epochs=10,
                             validation_data=validation_generator,
                             validation_steps=200,
                             verbose=1,
                             shuffle=1)

# Plotting training results
plt.plot(training_history.history['loss'])
plt.plot(training_history.history['val_loss'])
plt.legend(['Training', 'Validation'])
plt.title('Model Loss')
plt.xlabel('Epoch')

# Save the trained model
model.save('model.h5')

# Download the model file (specific to Google Colab environment)
from google.colab import files
files.download('model.h5')